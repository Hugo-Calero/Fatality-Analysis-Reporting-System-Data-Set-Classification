{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tDukbVB35fUH",
    "outputId": "09fc9376-5aac-42c8-cafe-95c461c46953"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J96dDFvfZrix"
   },
   "outputs": [],
   "source": [
    "# pip install sdmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-OyIZzaxkszv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas_profiling import ProfileReport\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.impute import KNNImputer\n",
    "# import mrmr\n",
    "# from mrmr import mrmr_classif\n",
    "# from sklearn_genetic import GAFeatureSelectionCV\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import cohen_kappa_score, make_scorer,confusion_matrix, matthews_corrcoef, f1_score, ConfusionMatrixDisplay\n",
    "import scipy.stats as stats\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks, RandomUnderSampler\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import sdmetrics\n",
    "from sdmetrics.reports.single_table import QualityReport\n",
    "import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UGL4vcWUaO3N"
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"/content/drive/MyDrive/data_tabular/fars.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WK1CWcdwMGkD"
   },
   "source": [
    "# Pre processing and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xMlZzc7PF_J2"
   },
   "outputs": [],
   "source": [
    "# Remove duplicated rows\n",
    "df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r5Ei8TVTnRoe"
   },
   "outputs": [],
   "source": [
    "# Define lists of continuous and categorical variables\n",
    "continuous = [' AGE', ' ALCOHOL_TEST_RESULT', ' DRUG_TEST_RESULTS_(1_of_3)', ' DRUG_TEST_RESULTS_(2_of_3)', ' DRUG_TEST_RESULTS_(3_of_3)']\n",
    "categorical = []\n",
    "for i in df.columns.to_list():\n",
    "    if i not in continuous:\n",
    "        categorical.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-TsEp2A1fbDM"
   },
   "outputs": [],
   "source": [
    "# Create NaNs for missing values for posterior imputation\n",
    "for var in [' SEX' ,' SEATING_POSITION', ' RESTRAINT_SYSTEM-USE',' EJECTION',' EJECTION_PATH',' EXTRICATION',' NON_MOTORIST_LOCATION', ' TAKEN_TO_HOSPITAL', ' RELATED_FACTOR_(1)-PERSON_LEVEL', ' RELATED_FACTOR_(2)-PERSON_LEVEL', ' RELATED_FACTOR_(3)-PERSON_LEVEL', ' RACE',' HISPANIC_ORIGIN']:\n",
    "    df[var] = df[var].replace('Unknown', np.nan) \n",
    "df[' AIR_BAG_AVAILABILITY/DEPLOYMENT'] = df[' AIR_BAG_AVAILABILITY/DEPLOYMENT'].replace('Unknown_(If_Airbag_Available)', np.nan)\n",
    "df[' POLICE_REPORTED_ALCOHOL_INVOLVEMENT'] = df[' POLICE_REPORTED_ALCOHOL_INVOLVEMENT'].replace('Unknown_(Police_Reported)', np.nan) # ;  ----- NO QUITARRRR\n",
    "df[' POLICE_REPORTED_ALCOHOL_INVOLVEMENT'] = df[' POLICE_REPORTED_ALCOHOL_INVOLVEMENT'].replace('Not_reported', np.nan) # ;  ----- NO QUITARRRR\n",
    "for var in [' DRUG_TEST_TYPE', ' DRUG_TEST_TYPE_(2_of_3)' , ' DRUG_TEST_TYPE_(3_of_3)']:\n",
    "    df[var] = df[var].replace('Unknown_if_Tested_for_Drugs', np.nan)\n",
    "for var in [' DRUG_TEST_TYPE', ' DRUG_TEST_TYPE_(2_of_3)' , ' DRUG_TEST_TYPE_(3_of_3)']:\n",
    "    df[var] = df[var].replace('Unknown_Test_Type', np.nan)\n",
    "for var in [' METHOD_OF_DRUG_DETERMINATION', ' METHOD_ALCOHOL_DETERMINATION', ' POLICE-REPORTED_DRUG_INVOLVEMENT']:\n",
    "    df[var] = df[var].replace('Not_Reported', np.nan)\n",
    "df[' POLICE-REPORTED_DRUG_INVOLVEMENT'] = df[' POLICE-REPORTED_DRUG_INVOLVEMENT'].replace('Reported_Unknown', np.nan)\n",
    "df['INJURY_SEVERITY'] = df['INJURY_SEVERITY'].replace('Unknown', 'Notknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0sxj5POBxbt"
   },
   "outputs": [],
   "source": [
    "# Encode categories\n",
    "for var in categorical:\n",
    "    df[var] = df[var].astype('category')\n",
    "    df[var] = df[var].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yE5getZmEVXq"
   },
   "outputs": [],
   "source": [
    "for var in categorical:\n",
    "    df[var] = df[var].replace(-1, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPZnl9EizLdo"
   },
   "outputs": [],
   "source": [
    "# Eliminate columns with a high percentage of missing values\n",
    "df.drop([' METHOD_OF_DRUG_DETERMINATION', ' METHOD_ALCOHOL_DETERMINATION', ' POLICE-REPORTED_DRUG_INVOLVEMENT'], axis = 1, inplace = True)\n",
    "for i in [' METHOD_OF_DRUG_DETERMINATION', ' METHOD_ALCOHOL_DETERMINATION', ' POLICE-REPORTED_DRUG_INVOLVEMENT']:\n",
    "    categorical.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fyGrx2DjOZBU"
   },
   "outputs": [],
   "source": [
    "# Deal with missing values;\n",
    "missing_value_imputation = 'knn'\n",
    "\n",
    "dff = df.copy()\n",
    "if missing_value_imputation == 'not':\n",
    "    dff = dff.replace(np.nan, 'Unknown')\n",
    "    # CONVERTIR UNKNOWN A NUMERICO\n",
    "    df = dff\n",
    "if missing_value_imputation == 'eliminate':\n",
    "    df = dff.dropna()\n",
    "if missing_value_imputation == 'freq':\n",
    "    for var in categorical:\n",
    "    dff[var].fillna(df[var].mode().iloc[0], inplace = True)\n",
    "    for var in continuous:\n",
    "    dff[continuous].fillna(df[continuous].median().iloc[0], inplace = True)\n",
    "    df = dff\n",
    "if missing_value_imputation == 'knn': \n",
    "    imputer = KNNImputer(n_neighbors=3, weights='uniform', metric='nan_euclidean')\n",
    "    imputer.fit(dff)\n",
    "    Xtrans = imputer.transform(dff)\n",
    "    df = pd.DataFrame(Xtrans, columns = dff.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-U_c3T0wRLK"
   },
   "outputs": [],
   "source": [
    "binary = []\n",
    "numerical = []\n",
    "for var in df.columns:\n",
    "    if len(pd.unique(df[var])) == 2:\n",
    "        binary.append(var)\n",
    "    else: \n",
    "        numerical.append(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIJckP62DE4I"
   },
   "outputs": [],
   "source": [
    "def Cramers_v(df, col1, col2):\n",
    "    a = pd.crosstab(df[col1], df[col2], margins = False)\n",
    "\n",
    "    X2 = stats.chi2_contingency(np.array(a), correction=False)[0]\n",
    "    N = np.sum(np.array(a))\n",
    "    minimum_dimension = min(np.array(a).shape)-1\n",
    "    result = np.sqrt((X2/N) / minimum_dimension)\n",
    "\n",
    "          return np.sqrt((X2/N) / minimum_dimension)\n",
    "\n",
    "# Function to study correlation between vairables, using different coefficients for different types of variables;\n",
    "def correlation_study(df, numerical, categorical):\n",
    "  '''\n",
    "  Generate correlation matrix considering different correlation coefficients for mixed type data. \n",
    "  Pearson: numerical-numerical\n",
    "  Point-biserial: numerical-binary/categorical\n",
    "  Matthews: binary-binary\n",
    "  Cramers V: categorical-categorical\n",
    "  '''\n",
    "    corr_vals = []\n",
    "    variables = categorical + numerical\n",
    "    for var1 in variables:\n",
    "        listvar1 = []\n",
    "        for var2 in variables:\n",
    "            if var2 in variables:\n",
    "                if var1 in numerical and var2 in numerical:\n",
    "                    corr, _ = stats.pearsonr(df[var1], df[var2])\n",
    "                    listvar1.append(corr)\n",
    "                    print('pearson',corr)\n",
    "                if var1 in numerical and var2 in categorical:\n",
    "                    pbc, pval = stats.pointbiserialr(df[var2], df[var1])\n",
    "                    listvar1.append(pbc)\n",
    "                    print('point bi', pbc)\n",
    "                if var1 in categorical and var2 in numerical:\n",
    "                    pbc, pval = stats.pointbiserialr(df[var2], df[var1])\n",
    "                    listvar1.append(pbc) \n",
    "                    print('pointbi', pbc)\n",
    "                if var1 in categorical and var2 in categorical:\n",
    "                    listvar1.append(Cramers_v(df, var1, var2))\n",
    "                    print('Cramer', Cramers_v(df, var1, var2))\n",
    "                corr_vals.append(listvar1)\n",
    "                # print(listvar1)\n",
    "    corrdf = pd.DataFrame(corr_vals, columns = variables)\n",
    "    corrdf['Var'] = variables\n",
    "    corrdf.set_index('Var', inplace = True)\n",
    "    corrdf.update(corrdf.abs())  \n",
    "    return corrdf, corr_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6MQuvZfIRtsv"
   },
   "outputs": [],
   "source": [
    "corr_df, corr_vals = correlation_study(df, continuous, categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BvlH9gbCAixd",
    "outputId": "906ee7e5-5a20-4dd7-bf8a-11ac7515088f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: ylabel='Var'>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.heatmap(corr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJmY8cu_dYNw"
   },
   "source": [
    "\n",
    "# Useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gnx-Bf-gbH8C"
   },
   "source": [
    "## Imbalance correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IkuPd_T1fIP"
   },
   "outputs": [],
   "source": [
    "# Function to evaluate the quality of synthetic data created using oversampling\n",
    "def compute_quality_metrics(df_real, df_synthetic, list_categorical_vars, return_score=False):\n",
    "    dict_aux = {}\n",
    "    for var_name in df_real.columns.values:\n",
    "        if var_name in list_categorical_vars:\n",
    "            dict_aux[var_name] = {'type': 'categorical'}\n",
    "        else:\n",
    "            dict_aux[var_name] = {'type': 'numerical'}\n",
    "    dict_metadata = {'fields': dict_aux}\n",
    "\n",
    "    report = QualityReport()\n",
    "    report.generate(df_real, df_synthetic, dict_metadata)\n",
    "    df_metrics = report.get_details(property_name='Column Shapes')\n",
    "\n",
    "    fig1 = report.get_visualization(property_name='Column Shapes')\n",
    "    plt.savefig('colshapes_report.png')\n",
    "    fig1.show()\n",
    "\n",
    "    fig = report.get_visualization(property_name='Column Pair Trends')\n",
    "    plt.savefig('colpairs_report.png')\n",
    "    fig.show()\n",
    "\n",
    "    if return_score:\n",
    "        qscore = df_metrics.iloc[:, -1].values\n",
    "        return qscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_dArMvtH77L"
   },
   "outputs": [],
   "source": [
    "# Functions for Class-imbalance correction\n",
    "def smote_oversample_all(X_train, y_train, binary,  quality_rep, return_score, sample_strategy):\n",
    "    data_real = X_train\n",
    "    data_real['y'] = y_train\n",
    "    #sample_strategy = {1:42116, 2:42116, 3:42116, 4:42116, 5:42116, 6:42116, 7:42116}\n",
    "    sm = SMOTE(sampling_strategy=sample_strategy, k_neighbors = 3, random_state=0, n_jobs = -1)\n",
    "    X_res, y_res = sm.fit_resample(X_train, y_train)\n",
    "    data_synthetic = X_res\n",
    "    data_synthetic['y'] = y_res\n",
    "    X_final = X_train.append(X_res, ignore_index = True)\n",
    "    y_final = y_train.append(y_res, ignore_index = True)\n",
    "    print(X_train.shape)\n",
    "    print(X_final.shape)\n",
    "    if quality_rep == True:\n",
    "        # plt.figure(figsize=(40,40))\n",
    "        quality_report = compute_quality_metrics(data_real, data_synthetic, binary, return_score=False)\n",
    "        return X_final, y_final, quality_report\n",
    "    else:\n",
    "        return X_final, y_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41Bhaiu8c_jP"
   },
   "outputs": [],
   "source": [
    "def random_undersampling_all(X_train, y_train, sample_strategy):\n",
    "    rus = RandomUnderSampler(sample_strategy) \n",
    "    X_rus, y_rus = rus.fit_resample(X_train, y_train)\n",
    "    return X_rus, y_rus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtpiVe9adBEA"
   },
   "outputs": [],
   "source": [
    "def undersample_tomek_links(X_train, y_train):\n",
    "    tl = TomekLinks(n_jobs = -1)\n",
    "    X_res, y_res = tl.fit_resample(X_train, y_train)\n",
    "    return X_res, y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbzXnJMLeV-K"
   },
   "outputs": [],
   "source": [
    "def hybrid_imbalance(X, y):\n",
    "    minn = y.value_counts()[3.0]\n",
    "    # Tomek-links undersampling on majority classes\n",
    "    X_pre_tomek = X.loc[y.isin([1.0, 4.0, 2.0, 5.0, 7.0])]\n",
    "    y_pre_tomek = y.loc[y.isin([1.0, 4.0, 2.0, 5.0, 7.0])]\n",
    "    X_tomek, y_tomek = undersample_tomek_links(X_pre_tomek, y_pre_tomek)\n",
    "    # random undersampling on majority classes\n",
    "    sample_strategy = {1.0:minn, 2.0:minn, 4.0:minn, 5.0:minn, 7.0:minn}\n",
    "    X_u, y_u = random_undersampling_all(X_tomek, y_tomek, sample_strategy)\n",
    "\n",
    "    # oversample minority classes\n",
    "    X_no_tomek = X.loc[y.isin([0.0, 3.0, 6.0])]\n",
    "    y_no_tomek = y.loc[y.isin([0.0, 3.0, 6.0])]\n",
    "    X_o, y_o = smote_oversample_all(X_no_tomek, y_no_tomek, sample_strategy = {0.0: minn, 3.0:minn, 6.0:minn})\n",
    "\n",
    "    # merge both\n",
    "    X_h = X_u.append(X_o)\n",
    "    y_h = y_u.append(y_o)\n",
    "\n",
    "    return X_h, y_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdyQYZLZauQx"
   },
   "outputs": [],
   "source": [
    "# X = df.drop('INJURY_SEVERITY', axis = 1)\n",
    "# y = df['INJURY_SEVERITY']\n",
    "\n",
    "# Xo, yo, r = smote_oversample_all(X, y, [], True, False, sample_strategy = {1:42116, 2:42116, 3:42116, 4:42116, 5:42116, 6:42116, 7:42116})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V71TkH7QbNOC"
   },
   "source": [
    "## Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-eEj_jmGkWA"
   },
   "outputs": [],
   "source": [
    "# Function for feature normalization;\n",
    "def scale_features(x_train, x_test):\n",
    "    scaler = StandardScaler()\n",
    "    x_train_raw = x_train.copy()\n",
    "    x_test_raw = x_test.copy()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = pd.DataFrame(data = scaler.transform(x_train), columns = list(x_train_raw.columns))\n",
    "    x_test = pd.DataFrame(data = scaler.transform(x_test), columns = list(x_train_raw.columns))\n",
    "\n",
    "    return x_train, x_test, x_train_raw, x_test_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSX3Rg97bSV5"
   },
   "source": [
    "## Select classifier and hyperparametr grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Bd9aZGfg-v0"
   },
   "outputs": [],
   "source": [
    "# Function to retrieve untrained classifier and hyperparameter grids, to then train;\n",
    "def get_hyperparams(classifier_name: str, priors, cost_sensitive, num_features, seed_value: int = 4242): \n",
    "    MAX_ITERS = 20000\n",
    "    dict_param_grid_classifier = {}\n",
    "    classifier = None\n",
    "\n",
    "    if classifier_name == 'nv':\n",
    "        if cost_sensitive == True:\n",
    "            dict_param_grid_classifier = {'class_prior': priors}\n",
    "        else:\n",
    "            dict_param_grid_classifier = {'class_prior': (1/8,1/8,1/8,1/8,1/8,1/8,1/8,1/8)}\n",
    "        classifier = ComplementNB()\n",
    "\n",
    "    if classifier_name == 'svm':\n",
    "        dict_param_grid_classifier = {'C': np.linspace(0.01, 10, 10),'gamma': ['auto'], 'kernel': ['rbf', 'linear', 'poly', 'sigmoid']} \n",
    "        if cost_sensitive == True:\n",
    "            dict_param_grid_classifier['class_weight'] = ['balanced']\n",
    "        classifier = SVC(max_iter=MAX_ITERS, random_state=seed_value)\n",
    "\n",
    "    elif classifier_name == 'dt':\n",
    "        dict_param_grid_classifier = {'max_depth': np.arange(1, 30, 1), 'min_samples_split': np.arange(2, 50, 2)}\n",
    "        if cost_sensitive == True:\n",
    "            dict_param_grid_classifier['class_weight'] = ['balanced']\n",
    "        classifier = DecisionTreeClassifier(random_state=seed_value)\n",
    "\n",
    "    elif classifier_name == 'rf':\n",
    "        dict_param_grid_classifier = {'max_depth': np.arange(1, 30, 3), 'n_estimators': np.arange(20, 50, 5),'min_samples_split': np.arange(2, 25, 5)}\n",
    "        if cost_sensitive == True:\n",
    "            dict_param_grid_classifier['class_weight'] = ['balanced']\n",
    "        classifier = RandomForestClassifier(n_jobs=-1, random_state=seed_value)\n",
    "\n",
    "    elif classifier_name == 'knn':\n",
    "        dict_param_grid_classifier = {'n_neighbors': np.arange(1, 20, 1)}\n",
    "        classifier = KNeighborsClassifier()\n",
    "  \n",
    "    elif classifier_name == 'lr':\n",
    "        dict_param_grid_classifier = {'penalty': ['l2', 'none']}\n",
    "        if cost_sensitive == True:\n",
    "            dict_param_grid_classifier['class_weight'] = ['balanced']\n",
    "        classifier = LogisticRegression(multi_class='multinomial', n_jobs=-1, random_state=seed_value)\n",
    "\n",
    "    elif classifier_name == 'mlp': # ---------------- Cambiar\n",
    "        np.random.seed(seed_value)\n",
    "        # random.seed(seed_value)\n",
    "        tf.random.set_seed(seed_value)\n",
    "        fit_kwargs = {\"optimizer\": \"adam\", \"learning_rate\": \"adaptive\", \"max_iter\": MAX_ITERS, \"batch_size\": 64, \"validation_fraction\": 0.15, \"verbose\": 0}\n",
    "        dict_param_grid_classifier = {'learning_rate_init':np.linspace(0.001, 1, 10),\n",
    "                                    'hidden_layer_sizes': [(num_features, 10), (num_features, 20), (num_features, 30), (num_features, 40), (num_features, 50), \n",
    "                                                           (num_features, 60), (num_features, 70), (num_features, 80), (num_features, 100)]}\n",
    "        classifier = MLPClassifier(**fit_kwargs)\n",
    "\n",
    "  \n",
    "\n",
    "    list_dicts_params_learning_curve = []\n",
    "\n",
    "    for key, value in dict_param_grid_classifier.items():\n",
    "        dict_param_lrc = {'param_name': key, 'param_range': value}\n",
    "        list_dicts_params_learning_curve.append(dict_param_lrc)\n",
    "\n",
    "    return classifier, dict_param_grid_classifier, list_dicts_params_learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BxWoVW4D3Cis"
   },
   "outputs": [],
   "source": [
    "cohen_scorer = make_scorer(cohen_kappa_score, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8RbI858bYIY"
   },
   "source": [
    "## Feature selection techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8y9rj4hsCEw"
   },
   "outputs": [],
   "source": [
    "# Function for Genetic algorithms for feature selection;\n",
    "def perform_genetic_fs(x_train, y_train, v_col_names, estimator_name, idx, priors):\n",
    "    model_regressor, param_grid_regressor, list_dict_params = get_hyperparams(estimator_name, priors, cost_sensitive = True, num_features = x_train.shape[1],seed_value= idx)\n",
    "    df_x_train = pd.DataFrame(x_train, columns=v_col_names)\n",
    "    ga_model = GAFeatureSelectionCV(estimator=model_regressor, cv=3, scoring=cohen_scorer, population_size=30, generations=12, n_jobs=-1, verbose=True, keep_top_k=2, elitism=True,) \n",
    "    ga_model.fit(x_train, y_train)\n",
    "    v_selected_features_idx = ga_model.best_features_\n",
    "    v_selected_features = df_x_train.loc[:, v_selected_features_idx].columns.values\n",
    "    return v_selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4bYyc2H8dHCT"
   },
   "outputs": [],
   "source": [
    "# Kruskal Wallis test for feature selection (filter method):\n",
    "def perform_kruskal(X_train, y_train):\n",
    "    selected = []\n",
    "    pre = X_train.copy()\n",
    "    pre['y'] = y_train\n",
    "    for feat in pre.columns:\n",
    "        if feat != 'y':  \n",
    "          group0 = pre[feat].loc[pre['y'] == 0.0]\n",
    "          group1 = pre[feat].loc[pre['y'] == 1.0]\n",
    "          group2 = pre[feat].loc[pre['y'] == 2.0]\n",
    "          group3 = pre[feat].loc[pre['y'] == 3.0]\n",
    "          group4 = pre[feat].loc[pre['y'] == 4.0]\n",
    "          group5 = pre[feat].loc[pre['y'] == 5.0]\n",
    "          group6 = pre[feat].loc[pre['y'] == 6.0]\n",
    "          group7 = pre[feat].loc[pre['y'] == 7.0]\n",
    "          _,p = stats.kruskal(group0,group1, group2, group3, group4, group5, group6, group7)\n",
    "          if p < 0.05:\n",
    "            selected.append(feat)\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCtc33HMdHHL"
   },
   "outputs": [],
   "source": [
    "def perform_mutualinfo(X,y):\n",
    "    # do calculations\n",
    "    mutual = mutual_info_classif(X, y, random_state = 8)\n",
    "    mutual /= np.max(mutual)\n",
    "\n",
    "    zipped_pairs = zip(list(mutual), list(X.columns))\n",
    "    mutualinfo_var = [x for _, x in sorted(zipped_pairs, reverse = True)]\n",
    "    mutualinfo_val = sorted(mutual, reverse = True)\n",
    "    selected = []\n",
    "    for idx,i in enumerate(mutualinfo_val):\n",
    "        if i >= 0.1:\n",
    "            selected.append(mutualinfo_var[idx])\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOO4oAQSbbt8"
   },
   "source": [
    "## Interpretability methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VeAdVM2EdO18"
   },
   "outputs": [],
   "source": [
    "# This code implements the Accumulated Local effects interpretability method. The SHAP method code is inside the function: \"validation_results\" in the next section.\n",
    "def plot_corresponding_ale_numerical_feature(df_ale_numerical, df_ale_numerical_raw, var_name, estimator_name, fes, type_over, iter, flag_save_figure=True):\n",
    "\n",
    "    df_ale_numerical = df_ale_numerical.reset_index()\n",
    "    df_ale_numerical_raw = df_ale_numerical_raw.reset_index()\n",
    "    first_std = df_ale_numerical.iloc[0, 1]\n",
    "    df_ale_numerical['lowerCI_95%'] = df_ale_numerical['lowerCI_95%'].fillna(first_std)\n",
    "    df_ale_numerical['upperCI_95%'] = df_ale_numerical['upperCI_95%'].fillna(first_std)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 3.1))\n",
    "    ax.plot(df_ale_numerical_raw[var_name], df_ale_numerical['eff'], ls='solid', color='blue')\n",
    "    ax.fill_between(df_ale_numerical_raw[var_name], df_ale_numerical['lowerCI_95%'], df_ale_numerical['upperCI_95%'], label='95% CI', alpha=0.3, color='grey', lw=2)\n",
    "    ax.set_xlabel(var_name, fontsize=12)\n",
    "\n",
    "    plt.xticks(fontsize=11)\n",
    "    plt.yticks(fontsize=11)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.5, linestyle='--')\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if flag_save_figure:\n",
    "        fig.savefig('{}_{}_{}_ale_{}_seed_{}.pdf'.format(estimator_name,type_over,fes,var_name,iter))\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def plot_corresponding_ale_categorical_feature(df_ale_cat, var_name, estimator_name, fes, type_over, iter, flag_save_figure=True):\n",
    "\n",
    "    df_ale_cat = df_ale_cat.reset_index()\n",
    "    first_std = df_ale_cat.iloc[0, 1]\n",
    "    df_ale_cat['lowerCI_95%'] = df_ale_cat['lowerCI_95%'].fillna(first_std)\n",
    "    df_ale_cat['upperCI_95%'] = df_ale_cat['upperCI_95%'].fillna(first_std)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 3.1))\n",
    "    ax.bar(df_ale_cat[var_name], df_ale_cat['eff'], width=0.4, color='blue', alpha=0.0)\n",
    "    ax.set_ylabel('', fontsize=16)\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.bar(df_ale_cat[var_name], df_ale_cat['size'], width=0.4, color='blue', alpha=0.3)\n",
    "    ax2.set_ylabel('Size', fontsize=16)\n",
    "    ax.plot([df_ale_cat.iloc[0, 0], df_ale_cat.iloc[1, 0]],[df_ale_cat.iloc[0, 1], df_ale_cat.iloc[1, 1]],'--', marker='o', color='black')\n",
    "\n",
    "    for index, row in df_ale_cat.iterrows():\n",
    "        p_inf = [row[var_name], row['lowerCI_95%']]\n",
    "        p_center = [row[var_name], row['eff']]\n",
    "        yerr = math.sqrt((p_center[0] - p_inf[0])**2 + (p_center[1] - p_inf[1])**2)\n",
    "        ax.errorbar(row[var_name], row['eff'], yerr, fmt='o', lw=2, capsize=4, color='black')\n",
    "\n",
    "    ax.set_xlabel(var_name, fontsize=12)\n",
    "    ax.set_xticks([0, 1])\n",
    "    plt.grid(alpha=0.5, linestyle='--')\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if flag_save_figure:\n",
    "        fig.savefig('{}_{}_{}_ale_{}_seed_{}.pdf'.format(estimator_name, type_over, fes, var_name, iter))\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def plot_ale_features(df_x_test, v_removed_features, model_estimator, df_x_test_raw, list_vars_numerical, list_vars_categorical, estimator_name, fes, type_over, iter):\n",
    "\n",
    "    list_vars_numerical = list(set(list_vars_numerical) - set(list(v_removed_features)))\n",
    "    list_vars_categorical = list(set(list_vars_categorical) - set(list(v_removed_features)))\n",
    "    print('esta es la lista', list_vars_categorical)\n",
    "    for var_numerical in list_vars_numerical:\n",
    "        df_ale_numerical = ale(X=df_x_test,model=model_estimator,feature=[var_numerical],plot=True,feature_type=\"continuous\",grid_size=10,include_CI=True,C=0.95)\n",
    "        df_ale_numerical_raw = ale(X=df_x_test_raw,model=model_estimator,feature=[var_numerical],plot=False,feature_type=\"continuous\",grid_size=10,include_CI=True,C=0.95)\n",
    "        plot_corresponding_ale_numerical_feature(df_ale_numerical, df_ale_numerical_raw, var_numerical,estimator_name, fes, type_over, iter)\n",
    "\n",
    "    for var_categorical in list_vars_categorical:\n",
    "        ale_discr = ale(X=df_x_test,model=model_estimator,feature=[var_categorical],feature_type=\"discrete\",plot=True,grid_size=10,include_CI=True,C=0.95)\n",
    "        plot_corresponding_ale_categorical_feature(ale_discr, var_categorical,estimator_name, fes, type_over, iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4HuZLHfbiei"
   },
   "source": [
    "## Evaluation of pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2IqlA0jgZ2s"
   },
   "outputs": [],
   "source": [
    "def validation_results(X, y, classifier,imputation_type, imbalance_correction_type, fs_method, cost_sensitive):\n",
    "\n",
    "  # Perform imbalance correction\n",
    "    if imbalance_correction_type == 'cost':\n",
    "        X = X\n",
    "        y = y\n",
    "    if imbalance_correction_type == 'hybrid':\n",
    "        X, y = hybrid_imbalance(X, y)\n",
    "\n",
    "    current_time = datetime.datetime.now()\n",
    "    kf = StratifiedKFold(n_splits=5, random_state = 42, shuffle=True)\n",
    "    training_scores = []\n",
    "    iter = 0\n",
    "    test_score_cohen = []\n",
    "    test_score_f1 = []\n",
    "    preds = []\n",
    "\n",
    "    actual_labels = []\n",
    "\n",
    "    for train_index,test_index in kf.split(X,y):\n",
    "        # Generate the training and test partitions of X and Y for each iteration of CV\t\n",
    "        X_train, X_test = X.iloc[list(train_index)], X.iloc[list(test_index)]\n",
    "        y_train, y_test = y.iloc[list(train_index)], y.iloc[list(test_index)]\n",
    "\n",
    "        # Feature Scaling\n",
    "        X_train_norm, X_test_norm, X_train_raw, X_test_raw = scale_features(X_train, X_test)\n",
    "        if classifier in ['dt', 'rf', 'xgb']:\n",
    "            X_train = X_train_raw\n",
    "            X_test = X_test_raw\n",
    "        else:\n",
    "            X_train = X_train_norm\n",
    "            X_test = X_test_norm\n",
    "        actual_labels.extend(y_test)\n",
    "\n",
    "        priors = np.array(y_train.value_counts()/len(y_train))\n",
    "\n",
    "        # Feature Selection\n",
    "        if fs_method == 'ga':\n",
    "            feature_selection = perform_genetic_fs(X_train, y_train, X_train.columns, classifier, iter, priors) \n",
    "        if fs_method == 'kw':  \n",
    "            feature_selection = perform_kruskal(X_train,y_train) #['CASE_STATE', ' AGE', ' SEX', ' PERSON_TYPE', ' SEATING_POSITION', ' RESTRAINT_SYSTEM-USE', ' AIR_BAG_AVAILABILITY/DEPLOYMENT', ' EJECTION', ' EJECTION_PATH', ' EXTRICATION', ' NON_MOTORIST_LOCATION',\n",
    "                              #' POLICE_REPORTED_ALCOHOL_INVOLVEMENT', ' ALCOHOL_TEST_TYPE', ' ALCOHOL_TEST_RESULT', ' DRUG_TEST_TYPE',       ' DRUG_TEST_RESULTS_(1_of_3)', ' DRUG_TEST_TYPE_(2_of_3)', ' DRUG_TEST_RESULTS_(2_of_3)', \n",
    "                              # ' DRUG_TEST_TYPE_(3_of_3)', ' DRUG_TEST_RESULTS_(3_of_3)', ' HISPANIC_ORIGIN', ' TAKEN_TO_HOSPITAL', ' RELATED_FACTOR_(1)-PERSON_LEVEL', ' RELATED_FACTOR_(2)-PERSON_LEVEL', ' RELATED_FACTOR_(3)-PERSON_LEVEL', ' RACE']\n",
    "        if fs_method == 'mrmr':\n",
    "            feature_selection = mrmr_classif(X=X, y=y, K=10)\n",
    "        if fs_method == 'mi':\n",
    "            feature_selection = perform_mutualinfo(X_train,y_train) #[' RACE',  ' HISPANIC_ORIGIN',  ' TAKEN_TO_HOSPITAL',  ' ALCOHOL_TEST_TYPE', ' ALCOHOL_TEST_RESULT',  ' DRUG_TEST_RESULTS_(1_of_3)',  ' DRUG_TEST_TYPE',  ' RESTRAINT_SYSTEM-USE']\n",
    "        if fs_method == 'corr':\n",
    "            feature_selection = [' SEX',' ALCOHOL_TEST_TYPE',' HISPANIC_ORIGIN',' TAKEN_TO_HOSPITAL',' RACE','INJURY_SEVERITY',' ALCOHOL_TEST_RESULT'] \n",
    "        else:\n",
    "            feature_selection = X_train.columns\n",
    "\n",
    "        X_train = X_train[feature_selection] \n",
    "        X_test = X_test[feature_selection] \n",
    "        # Get Model and Parameter grid_space\n",
    "        model_classifier, param_grid_classifier, list_dict_params = get_hyperparams(classifier, priors, cost_sensitive, num_features=X_train[feature_selection].shape[1], seed_value=iter)\n",
    "\n",
    "        # Nested 5-fold CV for hyperparameter tuning and validation results\n",
    "        grid_cv = GridSearchCV(estimator=model_classifier, param_grid=param_grid_classifier, scoring=cohen_scorer, cv=5, return_train_score=True, n_jobs=-1) #\n",
    "        grid_cv.fit(X_train, y_train)\n",
    "        best_estimator = grid_cv.best_estimator_\n",
    "        training_score = grid_cv.best_score_\n",
    "        training_scores.append(training_score)\n",
    "        iter = iter + 1\n",
    "\n",
    "        y_pred = best_estimator.predict(X_test)\n",
    "        preds.extend(y_pred)\n",
    "        test_score_cohen.append(cohen_kappa_score(y_test, y_pred))\n",
    "        test_score_f1.append(f1_score(y_test, y_pred, average = 'micro'))\n",
    "        cmiter=confusion_matrix(actual_labels,preds)\n",
    "\n",
    "        # SHAP Explainability\n",
    "        deita = shap.sample(X_test, 1000)\n",
    "        explainer = shap.KernelExplainer(best_estimator.predict, deita)\n",
    "        shap_values = explainer.shap_values(deita)\n",
    "        shap_expected = explainer.expected_value\n",
    "        shap_importance = (np.abs(shap_values).mean(0))\n",
    "        shap.summary_plot(shap_values, deita, show=False)\n",
    "\n",
    "        print('Test score for', classifier, 'model:', cohen_kappa_score(y_test, y_pred))\n",
    "\n",
    "        metrics_dict = {'Date':current_time, 'Iteration':iter, 'Imbalanced_correction':imbalance_correction_type , 'Missing_imputation':imputation_type, 'Classifier':classifier, 'FS':fs_method,\n",
    "                       'selected_feat':feature_selection, 'Best_params':grid_cv.best_params_, 'Training_score_iteration':grid_cv.best_score_, \n",
    "                       'Cohens_kappa_test':cohen_kappa_score(y_test, y_pred), 'F1_test':f1_score(y_test, y_pred, average = 'micro'), 'cm': cmiter}\n",
    "        df_metrics = pd.read_excel(\"/content/drive/MyDrive/data_tabular/metrics.xlsx\")\n",
    "        df_metrics = df_metrics.append(metrics_dict, ignore_index=True)\n",
    "        df_metrics.to_excel(\"/content/drive/MyDrive/data_tabular/metrics.xlsx\", index=False)\n",
    "\n",
    "    print('Mean training score for', classifier, 'model:', np.mean(training_scores))\n",
    "    print('Mean score: Cohens in 5 test partitions', np.mean(test_score_cohen), np.std(test_score_cohen))\n",
    "    print('Mean score: F1 in 5 test partitions', np.mean(test_score_f1), np.std(test_score_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZEtbrlFjH_j"
   },
   "source": [
    "# NO IMPUTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmhM5nEUjHJW"
   },
   "outputs": [],
   "source": [
    "X = df.drop('INJURY_SEVERITY', axis = 1)\n",
    "y = df['INJURY_SEVERITY']\n",
    "missing_value_imputation = 'none'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOHIw1f0xgcr"
   },
   "source": [
    "## No pre-processing, cost-sensitiive, no FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvmoLBafjMdn"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'lr', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='none', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7rtqnq0jMZC"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'knn', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='none', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dsnLx7zjMUd"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'rf', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='none', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X1Qg_xt4jMQH"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'dt', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='none', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6LLP58qxuTT"
   },
   "source": [
    "## No pre-processing, cost-sensitiive, KW FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esBEurhAxuTU"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'lr', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='kw', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zaICe93txuTV"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'knn', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='kw', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYXxeg88xuTV"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'rf', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='kw', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVZC6oAAxuTV"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'dt', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='kw', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiVuy71-xzrh"
   },
   "source": [
    "## No pre-processing, cost-sensitiive, MI FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJlSLLc4xzri"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'lr', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='mi', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBkiElPlxzrj"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'knn', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='mi', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evUhFoRGxzrj"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'rf', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='mi', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zxo5U9j3xzrj"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'dt', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='mi', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDLojOecx7xJ"
   },
   "source": [
    "## No pre-processing, hybrid, no FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckCVjiaMx7xK"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'lr', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='none', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LirCiY7x7xK"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'knn', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='none', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tiHJj5z9x7xL"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'rf', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='none', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETpKFH7Px7xL"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'dt', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='none', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OqWV0lAyG3N"
   },
   "source": [
    "## No pre-processing, hybrid, KW FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yMV6aYvXyG3O"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'lr', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='kw', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTOhhLxHyG3O"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'knn', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='kw', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ia7VRWQqyG3O"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'rf', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='kw', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWF6WaejyG3O"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'dt', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='kw', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziOID02qyMN1"
   },
   "source": [
    "## No pre-processing, hybrid, MI FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TecsonpWyMN2"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'lr', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='mi', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUb7ebkQyMN3"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'knn', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='mi', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHZt0RWMyMN3"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'rf', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='mi', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPLfXJjdyMN3"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'dt', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='mi', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRYmrwYooMxL"
   },
   "source": [
    "# FREQUENCY IMPUTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLvrBYmhN2Kx"
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"/content/drive/MyDrive/data_tabular/freq_imputation.xlsx\")\n",
    "X = df.drop('INJURY_SEVERITY', axis = 1)\n",
    "y = df['INJURY_SEVERITY']\n",
    "\n",
    "missing_value_imputation = 'freq'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65NWlcsgmIGS"
   },
   "source": [
    "## Frequency Imputation, cost-sensitive, no FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZo4feJHo7GS"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'lr', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='none', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-QeSZQxQ_g3"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'knn', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='none', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XNTCmcit3Vv"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'rf', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='none', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSjKaztK1ytb"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'dt', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='none', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzOsVsyQmOMK"
   },
   "source": [
    "## Frequency Imputation, cost-sensitive, KW FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pGakU36UpKu"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'lr', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='kw', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cweZjNDiUp2u"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'knn', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='kw', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEFnsGgBUp6I"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'rf', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='kw', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4-BhSQUUp9o"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'dt', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='kw', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvjlDnETmQtM"
   },
   "source": [
    "## Frequency Imputation, cost-sensitive, MI FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2DjDZuaoUqBj"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'lr', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='mi', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDPOq9VyU18x"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'knn', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='mi', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fna84GvqU2CD"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'rf', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='mi', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wf3taUPPU2G_"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'dt', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='mi', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZUcb1DFmUeP"
   },
   "source": [
    "## Frequency Imputation, hybrid, no FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hknz3qv2m5o9"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'lr', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='none', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5zD0qyBn6sA"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'knn', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='none', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNFxEiQen6sA"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'rf', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='none', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKmcYdPmn6sA"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'dt', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='none', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwHe-AZnmYsG"
   },
   "source": [
    "## Frequency Imputation, hybrid, KW FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHCSYXmynQ40"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'lr', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='kw', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyOhfthznQ40"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'knn', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='kw', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4AT0ODXnQ40"
   },
   "outputs": [],
   "source": [
    "#aquiiiiiiiiiiiiiiiiiiiiiiiiiiiiii\n",
    "validation_results(X,y,'rf', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='kw', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZuzm04-nQ40"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'dt', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='kw', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZW1q4mUSmYlY"
   },
   "source": [
    "## Frequency Imputation, hybrid, MI FS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WtxQ2cK1nXz2"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'lr', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='mi', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdZDYc17oDwn"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'knn', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='mi', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mzdAy2HmoDwn"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'rf', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='mi', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KErSS2y50PO9"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'dt', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='mi', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yD1Q0r44oRI2"
   },
   "source": [
    "# KNN IMPUTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQTW3h0IlS1q"
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"/content/drive/MyDrive/data_tabular/knn_imputation.xlsx\")\n",
    "X = df.drop('INJURY_SEVERITY', axis = 1)\n",
    "y = df['INJURY_SEVERITY']\n",
    "\n",
    "missing_value_imputation = 'knn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8q-ADU-mYaK"
   },
   "source": [
    "## KNN Imputation, cost-sensitive, no FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KG1_rti7nu1x"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'lr', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='none', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5IUsJ84Hnu1x"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'knn', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='none', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XI10LVaJnu1x"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'rf', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='none', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7sERGbAnu1y"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'dt', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='none', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBVvU4VDmnLk"
   },
   "source": [
    "## KNN Imputation, cost-sensitive, KW FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D77HTJchnxpo"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'lr', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='kw', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UtA9DrdZnxpo"
   },
   "outputs": [],
   "source": [
    "# aquiiiiiiiiiiiiiiiiiiiiiii\n",
    "validation_results(X,y,'knn', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='kw', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OgtCCxm5nxpp"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'rf', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='kw', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8VF3Hi7-nxpp"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'dt', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='kw', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQxmD-xCmnIc"
   },
   "source": [
    "## KNN Imputation, cost-sensitive, MI FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWnS1DkFn3uZ"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'lr', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='mi', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5rfd9-on3uZ"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'knn', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='mi', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAir9KKln3uZ"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'rf', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='mi', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebHjIxYIn3ua"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'dt', imputation_type = missing_value_imputation, imbalance_correction_type='cost', fs_method='mi', cost_sensitive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kb_ED2OImnFW"
   },
   "source": [
    "## KNN Imputation, hybrid, no FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wCSobcVBn8y6"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'lr', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='none', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCu6iCyDn8y6"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'knn', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='none', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vcqv5JFbn8y7"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'rf', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='none', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VgGW7Wx2n8y7"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'dt', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='none', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJzRUprHmnBH"
   },
   "source": [
    "## KNN Imputation, hybrid, KW FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wL5mzhEJoBAf"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'lr', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='kw', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SI0GGpO2oBAf"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'knn', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='kw', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uevnBfAjoBAf"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'rf', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='kw', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zwT16TksoBAg"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'dt', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='kw', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPIVt5vUmuZ1"
   },
   "source": [
    "## KNN Imputation, hybrid, MI FS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wxQ5v6rjoFw7"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'lr', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='mi', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRr8qqWanXz2"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'knn', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='mi', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bw2ShL1nXz3"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'rf', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='mi', cost_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ic6l-chjnXz3"
   },
   "outputs": [],
   "source": [
    "validation_results(X,y,'dt', imputation_type = missing_value_imputation, imbalance_correction_type='hybrid', fs_method='mi', cost_sensitive=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
